{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Analysis of source data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/spark\r\n"
     ]
    }
   ],
   "source": [
    "!which spark-submit\n",
    "!echo $SPARK_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.5\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fd6f5acc1fd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"Newyork_jobs_analysis\")\\\n",
    "        .getOrCreate()\n",
    "df = spark.read.csv(\"/dataset/nyc-jobs.csv\",header=True,inferSchema=True)\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "df.describe()\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA CLEANING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_columns(df):\n",
    "    for col_name in df.columns:\n",
    "        df = df.withColumnRenamed(col_name,col_name.lower().replace(\" \",\"_\" ))\n",
    "    return df\n",
    "\n",
    "df = clean_columns(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROCESSING DATE FUNCTION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dates(df):\n",
    "    df = df.withColumn(\"posting_date\", to_date(col(\"posting_date\"), \"MM/dd/yyyy\"))\n",
    "    df = df.withColumn(\"year\", year(col(\"posting_date\")))\n",
    "    df = df.withColumn(\"month\", month(col(\"posting_date\")))\n",
    "    return df\n",
    "\n",
    "df = process_dates(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average salary calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_salary(df):\n",
    "    \"\"\"\n",
    "    Compute avg_salary = (salary_range_from + salary_range_to) / 2\n",
    "    and return the values as a dataframe\n",
    "    \"\"\"\n",
    "    df = df.withColumn(\n",
    "        \"avg_salary\",\n",
    "        (col(\"salary_range_from\") + col(\"salary_range_to\")) / 2\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df = process_salary(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KPI 1  -- NUMBER OF JOBS PER CATEGORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_jobs_category = df.groupBy(\"job_category\")\\\n",
    "                    .count()\\\n",
    "                    .orderBy(desc(\"count\"))\\\n",
    "                    .limit(10)\n",
    "\n",
    "top_jobs_category.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KPI 2 -- SALARY DISTRIBUTION PER JOB CATEGORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_by_cateogry = df.groupBy(\"job_categrory\")\\\n",
    "                        .agg(avg(\"avg_salary\").alias(\"avg_salary\"))\\\n",
    "                        .orderBy(desc(\"avg_salary\"))\n",
    "salary_by_cateogry.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KPI 3 -- Correlation Between Degree & Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_degree():\n",
    "    return when(lower(col(\"minimum_qualifications\")).contains(\"phd\"), \"PhD\") \\\n",
    "        .when(lower(col(\"minimum_qualifications\")).contains(\"master\"), \"Masters\") \\\n",
    "        .when(lower(col(\"minimum_qualifications\")).contains(\"bachelor\"), \"Bachelors\") \\\n",
    "        .otherwise(\"No Degree\")\n",
    "\n",
    "df = df.withColumn(\"degree_level\", extract_degree())\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"degree_encoded\",\n",
    "    when(col(\"degree_level\") == \"PhD\", 3)\n",
    "    .when(col(\"degree_level\") == \"Masters\", 2)\n",
    "    .when(col(\"degree_level\") == \"Bachelors\", 1)\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "df.stat.corr(\"degree_encoded\", \"avg_salary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KPI 4 -- job posting having the highest salary per agency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##from pyspark.sql.functions import window\n",
    "w = window.partitionBy(\"Agency\").orderBy(desc(\"avg_salary\"))\n",
    "\n",
    "highest_salary_per_agency = (\n",
    "    df.withColumn(\"rank\", row_number().over(w))\n",
    "      .filter(col(\"rank\") == 1)\n",
    "      .select(\"agency\", \"business_title\", \"avg_salary\")\n",
    ")\n",
    "\n",
    "highest_salary_per_agency.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KPI 5 --  job positings average salary per agency for the last 2 yearS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_years_ago = datetime.now().year - 2\n",
    "\n",
    "df_recent = df.filter(col(\"year\") >= two_years_ago)\n",
    "\n",
    "avg_salary_last_2_years = (\n",
    "    df_recent.groupBy(\"agency\")\n",
    "             .agg(avg(\"avg_salary\").alias(\"avg_salary\"))\n",
    "             .orderBy(desc(\"avg_salary\"))\n",
    ")\n",
    "\n",
    "avg_salary_last_2_years.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KPI 6 -- highest paid skills in the US market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for this KPI I understand this can be done with Regex. As data doesnt have \n",
    "#skills column we can do it with the help of 2 different columns which is skills preferred and job desciption by exploding the values of it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE REMOVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    \"salary_range_from\",\n",
    "    \"salary_range_to\"\n",
    "]\n",
    "\n",
    "df_final = df.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WRITE TO FINAL TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.write.mode(\"overwrite\").parquet(\"nyc_jobs_processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Job ID: string (nullable = true)\n",
      " |-- Agency: string (nullable = true)\n",
      " |-- Posting Type: string (nullable = true)\n",
      " |-- # Of Positions: string (nullable = true)\n",
      " |-- Business Title: string (nullable = true)\n",
      " |-- Civil Service Title: string (nullable = true)\n",
      " |-- Title Code No: string (nullable = true)\n",
      " |-- Level: string (nullable = true)\n",
      " |-- Job Category: string (nullable = true)\n",
      " |-- Full-Time/Part-Time indicator: string (nullable = true)\n",
      " |-- Salary Range From: string (nullable = true)\n",
      " |-- Salary Range To: string (nullable = true)\n",
      " |-- Salary Frequency: string (nullable = true)\n",
      " |-- Work Location: string (nullable = true)\n",
      " |-- Division/Work Unit: string (nullable = true)\n",
      " |-- Job Description: string (nullable = true)\n",
      " |-- Minimum Qual Requirements: string (nullable = true)\n",
      " |-- Preferred Skills: string (nullable = true)\n",
      " |-- Additional Information: string (nullable = true)\n",
      " |-- To Apply: string (nullable = true)\n",
      " |-- Hours/Shift: string (nullable = true)\n",
      " |-- Work Location 1: string (nullable = true)\n",
      " |-- Recruitment Contact: string (nullable = true)\n",
      " |-- Residency Requirement: string (nullable = true)\n",
      " |-- Posting Date: string (nullable = true)\n",
      " |-- Post Until: string (nullable = true)\n",
      " |-- Posting Updated: string (nullable = true)\n",
      " |-- Process Date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"/dataset/nyc-jobs.csv\", header=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_salary_frequency(df: DataFrame) -> list:\n",
    "    row_list = df.select('Salary Frequency').distinct().collect()\n",
    "    return [row['Salary Frequency'] for row in row_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_data = [('A', 'Annual'), ('B', 'Daily')]\n",
    "expected_result = ['Annual', 'Daily']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_get_salary_frequency(mock_data: list, \n",
    "                              expected_result: list,\n",
    "                              schema: list = ['id', 'Salary Frequency']):  \n",
    "    mock_df = spark.createDataFrame(data = mock_data, schema = schema)\n",
    "    assert get_salary_frequency(mock_df) == expected_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
